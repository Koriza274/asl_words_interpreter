{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,Masking,Conv1D, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from functions.augmentation import augment_frame\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(video_path):\n",
    "    #video_path =os.path.join(\"raw_data/ASL/videos\",str(video_name))\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        landmarks_list = []\n",
    "\n",
    "\n",
    "        frame_index = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            #frame = normalize_frame(frame)\n",
    "            #frame = augment_frame(frame, flip=True, rotate=True, brightness=True)\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "                    landmarks_list.append(landmarks)\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    #cv2.imshow('landmarks',frame)\n",
    "                    #cv2.waitKey(200)\n",
    "\n",
    "        \"\"\"normalized_lm = []\n",
    "        for frame in landmarks_list:\n",
    "            # Extract wrist coordinates\n",
    "            wrist_x, wrist_y, wrist_z = frame[0]\n",
    "\n",
    "            # Normalize each landmark in the frame relative to the wrist\n",
    "            normalized_frame = []\n",
    "            for landmark in frame:  # Iterate over (x, y, z) coordinates\n",
    "                normalized_x = landmark[0] - wrist_x\n",
    "                normalized_y = landmark[1] - wrist_y\n",
    "                normalized_z = landmark[2] - wrist_z\n",
    "                normalized_frame.append((normalized_x, normalized_y, normalized_z))\n",
    "\n",
    "\n",
    "\n",
    "            normalized_lm.append(normalized_frame)\"\"\"\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(landmarks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks_augmented(video_path):\n",
    "    #video_path =os.path.join(\"raw_data/ASL/videos\",str(video_name))\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        landmarks_list = []\n",
    "\n",
    "\n",
    "        frame_index = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            #frame = normalize_frame(frame)\n",
    "            frame = augment_frame(frame, flip=True, rotate=True, brightness=False)\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "                    landmarks_list.append(landmarks)\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    #cv2.imshow('landmarks',frame)\n",
    "                    #cv2.waitKey(200)\n",
    "\n",
    "        \"\"\"normalized_lm = []\n",
    "        for frame in landmarks_list:\n",
    "            # Extract wrist coordinates\n",
    "            wrist_x, wrist_y, wrist_z = frame[0]\n",
    "\n",
    "            # Normalize each landmark in the frame relative to the wrist\n",
    "            normalized_frame = []\n",
    "            for landmark in frame:  # Iterate over (x, y, z) coordinates\n",
    "                normalized_x = landmark[0] - wrist_x\n",
    "                normalized_y = landmark[1] - wrist_y\n",
    "                normalized_z = landmark[2] - wrist_z\n",
    "                normalized_frame.append((normalized_x, normalized_y, normalized_z))\n",
    "\n",
    "\n",
    "\n",
    "            normalized_lm.append(normalized_frame)\"\"\"\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks_bright(video_path):\n",
    "    #video_path =os.path.join(\"raw_data/ASL/videos\",str(video_name))\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        landmarks_list = []\n",
    "\n",
    "\n",
    "        frame_index = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            #frame = normalize_frame(frame)\n",
    "            frame = augment_frame(frame, flip=False, rotate=False, brightness=True)\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "                    landmarks_list.append(landmarks)\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    #cv2.imshow('landmarks',frame)\n",
    "                    #cv2.waitKey(200)\n",
    "\n",
    "        \"\"\"normalized_lm = []\n",
    "        for frame in landmarks_list:\n",
    "            # Extract wrist coordinates\n",
    "            wrist_x, wrist_y, wrist_z = frame[0]\n",
    "\n",
    "            # Normalize each landmark in the frame relative to the wrist\n",
    "            normalized_frame = []\n",
    "            for landmark in frame:  # Iterate over (x, y, z) coordinates\n",
    "                normalized_x = landmark[0] - wrist_x\n",
    "                normalized_y = landmark[1] - wrist_y\n",
    "                normalized_z = landmark[2] - wrist_z\n",
    "                normalized_frame.append((normalized_x, normalized_y, normalized_z))\n",
    "\n",
    "\n",
    "\n",
    "            normalized_lm.append(normalized_frame)\"\"\"\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 459/459 [24:01<00:00,  3.14s/it]\n",
      "Processing videos: 100%|██████████| 459/459 [27:34<00:00,  3.60s/it] \n"
     ]
    }
   ],
   "source": [
    "lm_list = []\n",
    "\n",
    "path = 'raw_data/Citizen/videos'\n",
    "for item in tqdm(os.listdir(path),desc = 'Processing videos'):\n",
    "    item_path = os.path.join(path,item)\n",
    "    landmark = extract_landmarks(item_path)\n",
    "    lm_list.append(landmark)\n",
    "\n",
    "\"\"\"for item in tqdm(os.listdir(path),desc = 'Processing videos'):\n",
    "    item_path = os.path.join(path,item)\n",
    "    landmark = extract_landmarks_augmented(item_path)\n",
    "    lm_list.append(landmark)\"\"\"\n",
    "\n",
    "for item in tqdm(os.listdir(path),desc = 'Processing videos'):\n",
    "    item_path = os.path.join(path,item)\n",
    "    landmark = extract_landmarks_bright(item_path)\n",
    "    lm_list.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_padded = pad_sequences(lm_list,padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "\n",
    "for item in os.listdir(path):\n",
    "    item_path = os.path.join(path,item)\n",
    "\n",
    "    name = lambda x:re.sub(r'\\s|seed|\\d+|-', '', x[0:-4])\n",
    "\n",
    "    word_list.append(name(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_word_list = word_list+word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lms-bright.npy',landmarks_padded)\n",
    "np.save(\"labels-bright.npy\",full_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"labels-bright.npy\")\n",
    "X = np.load(\"lms-bright.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_encoded = to_categorical(y_encoded)\n",
    "label_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = np.array([x.reshape(x.shape[0], -1) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y_encoded, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734, 276, 63)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diana/.pyenv/versions/3.10.6/envs/asl_words_interpreter/lib/python3.10/site-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,680</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_11 (\u001b[38;5;33mMasking\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m, \u001b[38;5;34m63\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m327,680\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,864</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m526,864\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,864</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m526,864\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_length = X_train[0].shape[0]  # Number of time steps (max length)\n",
    "num_features = X_train[0].shape[1]    # This is now 63 (flattened features per time step)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Masking(mask_value=0.0, input_shape=(sequence_length, num_features)))\n",
    "\n",
    "#model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))  # First LSTM layer with sequences returned\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(LSTM(128, return_sequences=False))  # Second LSTM layer\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(16,activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = Adam(),metrics =[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.1922 - loss: 2.5477 - val_accuracy: 0.1973 - val_loss: 2.5501\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2707 - loss: 2.4340 - val_accuracy: 0.1973 - val_loss: 2.5133\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2297 - loss: 2.4651 - val_accuracy: 0.1905 - val_loss: 2.4807\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2613 - loss: 2.3943 - val_accuracy: 0.2177 - val_loss: 2.4784\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2422 - loss: 2.4255 - val_accuracy: 0.2177 - val_loss: 2.4618\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2523 - loss: 2.3487 - val_accuracy: 0.2313 - val_loss: 2.4501\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2799 - loss: 2.2638 - val_accuracy: 0.2313 - val_loss: 2.4116\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2835 - loss: 2.2556 - val_accuracy: 0.2449 - val_loss: 2.4098\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2716 - loss: 2.2691 - val_accuracy: 0.2245 - val_loss: 2.4281\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2721 - loss: 2.2797 - val_accuracy: 0.2245 - val_loss: 2.4149\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2956 - loss: 2.1868 - val_accuracy: 0.2177 - val_loss: 2.4045\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2702 - loss: 2.2358 - val_accuracy: 0.2245 - val_loss: 2.4352\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2898 - loss: 2.2678 - val_accuracy: 0.2041 - val_loss: 2.4656\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2938 - loss: 2.2036 - val_accuracy: 0.2177 - val_loss: 2.4350\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2859 - loss: 2.1789 - val_accuracy: 0.2381 - val_loss: 2.4463\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3086 - loss: 2.1546 - val_accuracy: 0.2517 - val_loss: 2.3996\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3106 - loss: 2.1303 - val_accuracy: 0.2449 - val_loss: 2.4125\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3332 - loss: 2.0823 - val_accuracy: 0.2517 - val_loss: 2.4127\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3663 - loss: 2.0155 - val_accuracy: 0.2449 - val_loss: 2.4045\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3486 - loss: 1.9913 - val_accuracy: 0.2313 - val_loss: 2.4560\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3377 - loss: 2.1067 - val_accuracy: 0.2449 - val_loss: 2.4419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4e189cfbb0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(patience = 5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train,y_train,validation_split = 0.2,epochs = 100,batch_size =32,callbacks=es)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_words_interpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
